{
	"nodes":[
		{"id":"6e35350197942402","type":"group","x":-320,"y":-240,"width":3080,"height":1160,"label":"Conversation"},
		{"id":"98c01d9a0ad2d39c","type":"group","x":-3820,"y":-240,"width":2780,"height":1240,"label":"Memory"},
		{"id":"05885145347a6339","type":"group","x":-5080,"y":2280,"width":2840,"height":920,"label":"Environment Interaction"},
		{"id":"7a12642b0b472d80","type":"group","x":-1670,"y":1640,"width":1261,"height":833,"label":"Planning"},
		{"id":"9dff3f036b464667","type":"text","text":"### Graph Traversal\nConversation Start\n- NPC should start with an intent for the conversation. Conversations can be started at will by the initiating NPC, provided the two NPCs are close enough\n- Conversation History: initialize conversation to empty\n- Event Retrieval: retrieve events based on initial intent and the other NPC\n- Social Memory: Summarize initial retrieved events for summary events. Retrieve preexisting relationship and feeling then update based on summary events if needed. \n- Response: based on intent and social memory, come up with a response\n- Conversation History: add response to conversation history\n- Event Retrieval: retrieve events based on initial intent, response, and other NPC\n- Social Memory: Integrate retrieved events into summary events, update relationship / feeling if needed\n- Other NPC's Response: request response from other NPC\n\nReply\n- Event Retrieval: retrieve events based on conversation history and the other NPC\n- Social Memory: Update summary events based on retrieved events. Initialize relationship/feeling similarly to conversation start if this is the first response. Update relationship/feeling if needed based on conversation history and summary events\n- Response: based on conversation history and social memory, come up with a response\n- Conversation History: add response to conversation history\n- Event Retrieval: retrieve events based on conversation history and other NPC\n- Social Memory: Integrate retrieved events into summary events, update relationship / feeling if needed\n- Other NPC's Response: request response from other NPC unless conversation is to be ended","x":2000,"y":-51,"width":680,"height":891},
		{"id":"b2404be5d4afba4c","type":"text","text":"### Other NPC's Response","x":560,"y":680,"width":320,"height":80},
		{"id":"7f5ee6b8779fcb89","type":"text","text":"### Social Memory\n- Relationship: how are the two agents connected? Eg., acquaintances, coworkers, friends, siblings, etc.\n- Feeling: valence of the relationship between the two agents\n- Summary Events: A summary of the retrieved events. After each conversational turn, integrate the new batch of retrieved into the current events summary","x":720,"y":-105,"width":520,"height":320},
		{"id":"c13b8fe8ed729c32","type":"text","text":"### Response\n- Decide whether to end the conversation\n- Given the social memory and conversation history, come up with a response","x":1440,"y":303,"width":421,"height":195},
		{"id":"a17e4c6f5b28ba75","type":"text","text":"### Implementation Notes\n- When retrieving memories, use LLM rerankers to make sure the most useful and relevant memories are returned","x":-1678,"y":584,"width":456,"height":234},
		{"id":"cea84a9c29d7d3af","type":"text","text":"### External Interface\n- Index reflections, observations, and textual spatial data using a text encoder.","x":-1520,"y":190,"width":342,"height":180},
		{"id":"f2fd2dd6bebf6614","type":"text","text":"### Conversation History\n- Provide in full for now, eventually summarize older conversational turns\n- Include initial intent for initiating NPC","x":-290,"y":401,"width":380,"height":183},
		{"id":"eff71ef1a9d0fad1","type":"text","text":"### Text Memory Table\n- Store observations, reflections, and textual elements of spatial memory in a single table\n- For computing a semantic embedding, use an instruction-finetuned text encoder like Instructor-Large. This will allow greater flexibility in crafting queries\n- Columns should include `id`, `memory_text`, `memory_type` (observation, reflection, or spatial), `semantic_embedding`, `locational_embedding` (save the current location embedding, since it will change in the future), `associated_npcs`, `timestamp`, `location`, `importance`","x":-2240,"y":63,"width":480,"height":425},
		{"id":"c21d5208fc49e4b7","type":"text","text":"### Spatial Memory Table\n- This table should represent a directed acyclic graph, where locations are represented by nodes and containership is represented by an edge from parent to child. \n- Locations should have embeddings. This embedding will have two different weighted components:\n\t- **Individual Component:** The NPC recalls its top relevant memories about the location, then sets this embedding component to the average of the recalled memory embeddings.\n\t- **Neighbor Component:** This component should be the weighted average of the individual embedding components of the node's parent and child nodes\n- Columns should include `id`, `location_name`, `parent_location`, `child_locations`, `embedding`, `individual_embedding`","x":-2980,"y":352,"width":500,"height":507},
		{"id":"709ea8499b684dfd","type":"text","text":"### Reflection\n- Synthesized from multiple related observations and/or reflections","x":-2980,"y":111,"width":300,"height":197},
		{"id":"b0f4d339ca9f1e51","type":"text","text":"### Action Interface\n- Pass chosen actions to the simulation.\n- Relay success/error/status messages from the simulation to the NPC","x":-3369,"y":2472,"width":301,"height":217},
		{"id":"45324268ee94558b","type":"text","text":"### NPC Status UI\n- If NPC is selected, pull updates from the NPC to populate the visible status UI elements","x":-2640,"y":2450,"width":300,"height":190},
		{"id":"f8e83984b3a5a32a","type":"text","text":"### Decision Procedure","x":-3668,"y":1280,"width":459,"height":309},
		{"id":"23d44a678e649619","type":"text","text":"### Terrain","x":-4040,"y":2840,"width":280,"height":240},
		{"id":"fbe5c4aa8368a3f3","type":"text","text":"### Vision\n- TBD, but could use a vision-language model","x":-3680,"y":2840,"width":262,"height":154},
		{"id":"69b956621c8c36b1","type":"text","text":"### Sense Aggregator\n- Combine text data from each sensor modality into a single formatted string (perhaps JSON or XML).","x":-4200,"y":2480,"width":356,"height":200},
		{"id":"d428fc4219e5acca","type":"text","text":"- Retrieve memories with high semantic similarity to what is being discussed in the conversation, and particularly to the latest turn of conversation\n- Boost results by associated NPCs","x":-800,"y":190,"width":335,"height":211},
		{"id":"96cbb2160d828c83","type":"text","text":"### Event Retrieval\n- Retrieve events from the the NPC's memory which are relevant to the interlocuters and the events being discussed","x":90,"y":-80,"width":320,"height":270},
		{"id":"560ca06b7c430877","type":"text","text":"### Observation\n- Low level observation","x":-2980,"y":-97,"width":360,"height":160},
		{"id":"1ed0321b48740629","type":"text","text":"### Working Memory\nFirst get a list of retrieved memories\n- Each memory and each sensory input should have an associated embedding. Let the set of all sensory input and memory embeddings from the previous working memory be $E = \\{e: e \\in \\mathbb{R}^d\\}$. Construct a set of random subsets, $S=\\{s: s \\subset E\\}$. Create a query vector using $q(s, w)=\\sum_{1}^{|s|} s_i w_i$, where $s \\in S$ and $w\\in \\mathbb{R}^{|s|}$. We can compute the weights $w$ using some randomness and some other coefficients derived from the memories' importances, relevances to the sensory input, etc. Then memories can be directly retrieved using the query vector $q(s, w)$.\n- Have an LLM craft search queries based on the summarized sensory input and the previous timestep's working memory. Boost based on recency, importance, semantic relevance, and locational relevance\n- Inject some randomness. The magnitude of this randomness should be parameterized so that it can vary based on the internal state of the NPC\n\nThe context will now consist of retrieved memories, a summary of the context of the previous working memory (prompt LLM to preserve any important information not already reflected in the current context), and a report on relevant sensory inputs / available actions\n\nNext summarize / synthesize any patterns from the context and add to the context","x":-3739,"y":-43,"width":600,"height":703},
		{"id":"bb866190f2a1055f","type":"text","text":"*Not* sure where to put this\n\nWhile sleeping, work on consolidating memories. Collate what happened during the day into a daily report. Every week, collate daily reports into a weekly report. Do something similar monthly, seasonally, and yearly.\n- Make sure to schedule these reports so that there is never more than two reports to write per night.\n- When referencing a lower level memory or memory report, cite it so that the agent can pinpoint specific memories using the citation graph\n- Each report should have a short summary\n- Index the summary and chunks of the whole document","x":-3200,"y":-760,"width":520,"height":380},
		{"id":"e0344aee36c8afe9","type":"text","text":"### Seen Locations\n- The subtree of the global location tree which consists of all visible locations and their parents","x":-4480,"y":2840,"width":360,"height":240},
		{"id":"a6b89b264a7dc8ab","type":"text","text":"### Sensory Processor\n- Raw sensory inputs should be passed through a processing step.\n- The output of the processing step:\n\t- A list of low level sensory inputs which should be directly passed to the NPC (LLM should justify why each one is helpful/important). This is like an extractive summary.\n\t- A list of higher-level insights / patterns (but still at the level of perceiving, not reflecting) from the low level sensory data. This is like an abstractive summary.\n\t- Possibly chunk + index the input (with text embeddings + some categorical filters) and have an LLM pose search queries to retrieve relevant content","x":-4878,"y":104,"width":518,"height":456},
		{"id":"761315bde89be97d","type":"text","text":"### Seen Objects\n- A list of each object and any related information","x":-4880,"y":2840,"width":311,"height":240},
		{"id":"443bb3857d0f4711","type":"text","text":"### Action Space\n- List of available interfaces, along with documentation for each.","x":-5000,"y":2487,"width":369,"height":187},
		{"id":"e0502af945765222","x":-2680,"y":-570,"width":480,"height":190,"type":"text","text":"When implementing retrieval, play around with different combinations of:\n- Averaging embeddings of multiple queries\n- query decomposition\n- Hyde retrieval"}
	],
	"edges":[
		{"id":"f2efd41322d819be","fromNode":"f2fd2dd6bebf6614","fromSide":"right","toNode":"c13b8fe8ed729c32","toSide":"left","label":"response needed?"},
		{"id":"1973d922339ee031","fromNode":"7f5ee6b8779fcb89","fromSide":"right","toNode":"c13b8fe8ed729c32","toSide":"left","label":"response\nneeded?"},
		{"id":"c215669f1a7a60f4","fromNode":"c13b8fe8ed729c32","fromSide":"bottom","toNode":"f2fd2dd6bebf6614","toSide":"right","label":"update history"},
		{"id":"24000154c5255384","fromNode":"f2fd2dd6bebf6614","fromSide":"top","toNode":"96cbb2160d828c83","toSide":"left","label":"new turn added?"},
		{"id":"188e302af9acbcbe","fromNode":"96cbb2160d828c83","fromSide":"right","toNode":"7f5ee6b8779fcb89","toSide":"left","label":"integrate\ninto"},
		{"id":"9dbadec273968744","fromNode":"c13b8fe8ed729c32","fromSide":"top","toNode":"7f5ee6b8779fcb89","toSide":"right","label":"update"},
		{"id":"8064a08f6d2a3163","fromNode":"c13b8fe8ed729c32","fromSide":"bottom","toNode":"b2404be5d4afba4c","toSide":"right","label":"send to other NPC"},
		{"id":"61af3529ca2d3823","fromNode":"b2404be5d4afba4c","fromSide":"left","toNode":"f2fd2dd6bebf6614","toSide":"bottom"},
		{"id":"139079091ede8340","fromNode":"1ed0321b48740629","fromSide":"right","toNode":"560ca06b7c430877","toSide":"left","label":"create"},
		{"id":"4f61d068466bddcb","fromNode":"cea84a9c29d7d3af","fromSide":"right","toNode":"d428fc4219e5acca","toSide":"left"},
		{"id":"e2e11e7b5147577f","fromNode":"d428fc4219e5acca","fromSide":"right","toNode":"6e35350197942402","toSide":"left"},
		{"id":"44eb13f299a64244","fromNode":"a6b89b264a7dc8ab","fromSide":"right","toNode":"1ed0321b48740629","toSide":"left"},
		{"id":"1a08f19128cfaf2c","fromNode":"69b956621c8c36b1","fromSide":"top","toNode":"a6b89b264a7dc8ab","toSide":"bottom","label":"sensory input"},
		{"id":"87204365a0d89a00","fromNode":"1ed0321b48740629","fromSide":"right","toNode":"c21d5208fc49e4b7","toSide":"left","label":"update"},
		{"id":"67e53a1bae39e2bd","fromNode":"1ed0321b48740629","fromSide":"right","toNode":"709ea8499b684dfd","toSide":"left","label":"create"},
		{"id":"2ad4c6e8721ecc96","fromNode":"560ca06b7c430877","fromSide":"right","toNode":"eff71ef1a9d0fad1","toSide":"left","label":"store"},
		{"id":"0c08e87607c6a427","fromNode":"709ea8499b684dfd","fromSide":"right","toNode":"eff71ef1a9d0fad1","toSide":"left","label":"store"},
		{"id":"082c7e256a912673","fromNode":"c21d5208fc49e4b7","fromSide":"right","toNode":"eff71ef1a9d0fad1","toSide":"left","label":"update"},
		{"id":"7ae2297673055083","fromNode":"eff71ef1a9d0fad1","fromSide":"right","toNode":"cea84a9c29d7d3af","toSide":"left"},
		{"id":"aa9635cf2cf76304","fromNode":"1ed0321b48740629","fromSide":"bottom","toNode":"f8e83984b3a5a32a","toSide":"top"},
		{"id":"bde9bb8162e64d2b","fromNode":"f8e83984b3a5a32a","fromSide":"bottom","toNode":"b0f4d339ca9f1e51","toSide":"top","label":"choose action"},
		{"id":"077a3d96e42f4305","fromNode":"443bb3857d0f4711","fromSide":"right","toNode":"69b956621c8c36b1","toSide":"left","label":"available\nactions"},
		{"id":"954975b540c06cb8","fromNode":"b0f4d339ca9f1e51","fromSide":"left","toNode":"69b956621c8c36b1","toSide":"right","label":"messages"},
		{"id":"8e76b4b34fe26f5d","fromNode":"98c01d9a0ad2d39c","fromSide":"bottom","toNode":"45324268ee94558b","toSide":"top"}
	]
}