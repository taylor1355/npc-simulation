This document explores why I scrapped the earlier text-based simulation in favor of the current 2D one implemented in Godot.
# Initial Implementation
## Ungroundedness Problem
In the previous version, most of the "mental burden" of maintaining an internally consistent world with interesting interactions fell on the gamemaster, with very barebones hardcoded mechanics. This made it really difficult to simulate even simple interactions. I think a good way to escape this trap would be to impose more structure on the game world, while still giving the AI gamemaster a high degree of creative freedom within the existing structure. 

### Object Library
Every item should have some prebuilt functionality (as opposed to relying on the gamemaster to facilitate every interaction). For example, maybe a dice has below built in logic, which will 
``` python
import random

class Die:
	def __init__(self,
		side_values: list[str], # the values shown on each side of the dice
		face_up_value: str
	):
		self.side_values = side_values
		self.face_up_value = face_up_value
		self.gamemaster_guidance = "\n".split([
			f"This item represents a {len(side_values)}-sided playing die.",
			f"- The different values on the sides are: {str(side_values)}",
			f"- The current side facing up has value: {face_up_value}"
		])
		
	def roll(self):
		self.face_up_value = self.side_values[random.randint(0, len(self.side_values))]

	def advise_gamemaster(self):
		return {
			"name": "die",
			"description": f"A {len(self.side_values)}-sided die. List of values on the sides: {self.side_values}",
			"status": f"The {self.face_up_value} side is currently facing up",
		}
```
or to abstract a bit:
```
# TODO: item classes should inherit from some 2D video game engine object. This way it will have physical properties like mass, material/texture, position, etc and can utilize them it its logic.
class <item>:
	# TODO: each item property should 
	def __init__(self, <item properties>):
		pass

	<functions which serve as interfaces for agents to interact with this item>

	def advise_gamemaster(self):
		return: {
			"name": <name of item>,
			"description": <summary of the general characteristics of the item>,
			"status": <summary of the current state of the item>,
			<optional other fields that would help gamemaster>
		}
		
```
It would be interesting to decouple the logic of an entity with the intelligence that drives it. For example, maybe an NPC is comprised of an AI which is "driving" a body through an interface similar to the above. This would simplify interactions with entities, as the acting agent would just need to use an interface. Interfaces should be fairly high-level, to allow the agents to focus on the big picture. For example, maybe maybe an agent wants to move from the living room to the kitchen. Instead of requiring them to choose how to move their legs or even figure out exact coordinates, there should probably be some sort of hierarchical point-of-interest pathfinding algorithm that uses a lower level interface to move the agent to the general location / object they wanted to navigate to.

In the simulation, agents can choose to use the interface provided by an item, or to interact in a novel way with the item. If they choose to interact in a novel way, then the gamemaster will facilitate the interaction.
- It may choose to extend the prebuilt functionality of the items involved in the interaction if a) adding the functionality would be likely to save future LLM calls and b) adding the functionality as code in the item class would be a relatively minor change (although in the case of a major change the gamemaster can add to / edit a list of requested features to a human in the loop). It should make sure to add comprehensive unit tests to reduce the failure rate. The main advantage of this approach is that it should reduce the LLM call burden over time. However, any flawed code would stick around, which could lead to compounding errors
- If the gamemaster decides not to extend the current functionality of the items then it can instead implement the interaction by directly manipulating the simulation state

When creating items, the gamemaster can see a library of all created objects. They can either use an off the shelf object, modify it themselves, or create an entirely new object. For an initial implementation, start with them only being able to instantiate or modify existing items. Here are some useful items to prepackage for the gamemaster:
- Generic NPC template - this has some simple options to configure and can create a background character for use in a particular situation
- Generic item templates - to reduce the development burden on the gamemaster
	- Container - could be modified to create a refrigerator, chest, bag, etc
	- Handheld object
	- Computer - could be modified to create other interactable devices
- Common items
	- Plants
	- Furniture
### Action Library
In order to ensure gamemaster conduct is consistent, a library of historic interventions should be saved. This can be indexed using vector search as well as tag based search. The gamemaster should respect historical precedent when carrying out actions

## Stingy LLM Calls Problem
I spent too much of my time trying to design a system that would minimize wasteful LLM calls. There are several different thought systems which each require additional LLM calls, so it is costly to give an NPC time to think every single timestep. Instead, I designed an event-based system where there would be dynamic gaps in time between decision points where the NPC would have to use its thought systems. This in theory could have saved some time, but in the long run it seems like it would introduce more complexity to the development process than it's worth. This is especially true given how quickly LLMs are improving in throughput for a given level of intelligence.

A compromise between the event-based system and a system in which each NPC is allocated thinking time every timestep may be the best option. Each NPC can have a lightweight routing routine which runs every timestep and decides which cognitive modules should be run for the current timestep, if any. Some cognitive modules will be regularly scheduled rather than being run exclusively on demand. Heavy offline processing will be spread out over the NPC's sleep cycle.

# Stretch Goals
## Aimlessness Problem
There should also be more of a sense of purpose driven by the gamemaster. It's tedious having to micromanage the environment and NPCs to do interesting things.

Instead, the gamemaster should be able to receive general instructions about how the simulation should proceed For example, maybe there is a particular narrative arc that the user wants to unfold over some time period, or maybe the user wants a historically accurate depiction of a medieval peasant family living and working on their farm. The gamemaster should be able to act by:
- Modifying the thought process of an NPC as it occurs. For example, if an NPC programmed to be a medieval peasant thinks about how much they want a Big Mac, the gamemaster can instead substitute a craving for some hearty stew.
	- For implementation of this feature, a high recall classifier (yes/no whether gamemaster should intervene given instructions) should be used to reduce how often the gamemaster is needed
- Affecting the physical environment. This should be only be done as a result of some agent taking an action. An agent could include a player, an NPC, or a non-conscious agent that still needs a complex inner state and/or behavior. Limit the action space to reduce complexity. The current implementation of just having the gamemaster edit the global state isn't sustainable.
	- Simple scripted interaction. This is when one agent interacts with another using a predefined interface. Some examples:
		- An NPC opening or closing a door (directly affects the open/closed property)
		- An NPC interacting with a Sims-esque interface to use the computer (send message to other NPC, play a game, work, etc)
	- Complex scripted  interaction. Let's say an NPC calls 911 on their phone. The gamemaster will then create temporary NPCs to play the operator and emergency responders. Here are some of the less straightforward consequences from 
		- Entity creation. This can include either a new NPC or non-conscious entity.

When the gamemaster makes an intervention, a record of the intervention will go into a list of incidents to be reviewed occasionally. Upon review, the gamemaster may decide to make more long-term changes to reduce the likelihood of future interventions. 
- The gamemaster could modify fields of the NPC's internal mental state or even add to the NPCs training data and have its finetuning be redone periodically


## Hard to Customize NPCs Problem
It is a lot of effort to prompt engineer NPCs that will act in a particular way fitting with the gamemaster's goals. It would be much easier to let the gamemaster do that work.

Train each NPC's LLM using reinforcement learning. A few simulations can be run with the starting NPCs (use domain randomization to increase generalization) before the final one is run. When new NPCs are added, there can be an option to pause the simulation and finetune them using the gamemaster. If they are less important, the gamemaster can just supply them with prompts to use.